# Real-Time Change Data Capture (CDC) Pipeline

This project demonstrates the implementation of a real-time CDC pipeline using **Postgres**, **Debezium**, **Kafka**, **NiFi**, and **Elasticsearch**. The primary goal is to efficiently capture and propagate changes made to a transactions table in a Postgres database to other systems for updates tracking and data synchronization.

---

## Project Overview

1. **Source Database (Postgres):**  
   - A Postgres database serves as the primary data source, containing a transactions table where changes (inserts, updates, deletes) occur.

2. **Debezium:**  
   - Debezium is configured as a Kafka Connect source connector to monitor the Postgres database's WAL (Write-Ahead Log) for changes in the transactions table.  
   - Each change is captured in near real-time and published as an event to a Kafka topic.

3. **Kafka:**  
   - Acts as a messaging layer, ensuring reliable delivery of the change events generated by Debezium.  
   - A dedicated topic is created for the transactions table to store these events.

4. **NiFi:**  
   - NiFi consumes the change events from the Kafka topic using a **ConsumeKafka_2_0** processor.  
   - The data is parsed, and relevant update details.  
   - NiFi workflows transform and route the data to the appropriate destinations.

5. **Elasticsearch:**  
   - The transformed data is indexed in Elasticsearch for real-time updates tracking and querying capabilities.  
   - This enables efficient visualization and historical analysis of changes in the transactions data.

6. **Secondary Database:**  
   - The processed updates are also written to another database, ensuring data consistency and enabling downstream applications to access the synchronized data. 

---

## Features

- **Real-Time Data Streaming:** Capture and propagate data changes with low latency.  
- **Scalable Architecture:** Leverages Kafka for high throughput and reliability.  
- **Data Transformation:** Processes and enriches change events in NiFi before routing them to target systems.  
- **Updates Tracking:** Stores change logs in Elasticsearch, enabling queryable and auditable records of all updates.  
- **Multi-Destination Integration:** Supports synchronization to multiple systems, ensuring consistent and up-to-date data across the ecosystem.

---

## Technologies Used

- **Postgres:** Source database for transactions.  
- **Debezium:** Change Data Capture tool for Postgres.  
- **Kafka:** Message broker for event-driven data transfer.  
- **NiFi:** Data flow automation and processing platform.  
- **Elasticsearch:** Distributed search and analytics engine.

---

## How It Works

1. Changes in the transactions table are recorded in the Postgres WAL.  
2. Debezium captures the WAL entries and publishes them to a Kafka topic as change events.  
3. NiFi consumes the events, processes them, and extracts meaningful update details.  
4. The processed data is routed to:  
   - A secondary database for data synchronization.  
   - Elasticsearch for tracking and querying changes.

---

This project showcases a practical use case of CDC for real-time data integration and monitoring. The architecture ensures scalability, reliability, and adaptability to various data-driven scenarios.
